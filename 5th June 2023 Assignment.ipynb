{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b1a97f-6614-49b3-add5-fde67eee88db",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b6e97-d4e1-4830-a909-3a66b8f1aabb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c874d5ad-e138-4a1f-bab6-378e6a77602a",
   "metadata": {},
   "source": [
    "## Q1. What is an activation function in the context of artificial neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881fa2c-e18b-42c9-bd8f-961ac679aa0b",
   "metadata": {},
   "source": [
    "-> Activation function is a mathematical function applied to a output of neuron.\n",
    "\n",
    "-> Role of activation function is to add non-linearity into the network.\n",
    "\n",
    "-> It can create impact on the training process of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39948e98-34d3-42c3-a341-cdbe432b13cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4900238-93d4-452f-8802-dc2b80eebeea",
   "metadata": {},
   "source": [
    "## Q2. What are some common types of activation functions used in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439bf4de-fe00-4aa4-8436-c7c1a56670bd",
   "metadata": {},
   "source": [
    "Sigmoid\n",
    "\n",
    "ReLU\n",
    "\n",
    "Leaky ReLU\n",
    "\n",
    "tanh\n",
    "\n",
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92024dac-8aa9-4a38-8172-a9b48fffea48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54d76ec2-2c8a-4899-ac1e-3247826de312",
   "metadata": {},
   "source": [
    "## Q3. How do activation functions affect the training process and performance of a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a1b0a-533c-4b2e-940b-7c9129903de4",
   "metadata": {},
   "source": [
    "-> The main objective of activation function is to intorduce non-linearity into the model.It makes out model more effective.\n",
    "\n",
    "-> In ReLU activation function there is a problem of a vanishing gradient,we have a varient of a ReLU that don't let the gradient vanish.\n",
    "\n",
    "-> Activation functions like ReLU and its variants LeakyReLU & PReLU helps in faster convergence.\n",
    "\n",
    "-> It also creates impact on accuracy and helps to make a generalized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef14db8-ce2f-44d0-a7f0-bee833fef32e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c07b0ad7-725c-487d-a030-e8a00e4eea3b",
   "metadata": {},
   "source": [
    "## Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd74d99-4870-45c7-8a7c-6eeadad3e43f",
   "metadata": {},
   "source": [
    "-> The sigmoid function squashes input values into a range where large positive numbers become close to 1, and large negative numbers become close to 0.\n",
    "\n",
    "-> Formuls : 1 / (1 + e**-z)\n",
    "\n",
    "* Advantages\n",
    "\n",
    "-> The output values are in the range (0, 1), making it suitable for binary classification problems.\n",
    "\n",
    "-> Sigmoid has a smooth gradient, which helps in gradient based optimization technique such as backpropagation.\n",
    "\n",
    "* Disadvantages\n",
    "\n",
    "-> Gradient of Sigmoid becomes close to 0, when it get the input value very high or very low.It can became a cause of slove convergence process, because of insignificance of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27294ece-83c0-4d80-abc1-ffc86fa9b60f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29937d4f-3b06-4e6f-b400-620961f417aa",
   "metadata": {},
   "source": [
    "## Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a30069-9739-4605-873c-14feb3932556",
   "metadata": {},
   "source": [
    "**ReLU(x) = max(0,x)**\n",
    "\n",
    "* **Derivative** \n",
    "\n",
    "1 if x > 0\n",
    "0 if x <= 0\n",
    "\n",
    "* **Difference betweeen Sigmoid and ReLU**\n",
    "\n",
    "1. **Range**\n",
    "\n",
    "Sigmoid : It ranges between 0 to 1.\n",
    "\n",
    "ReLU : It ranges between 0 to infinity\n",
    "\n",
    "2. **Gradient**\n",
    "\n",
    "Sigmoid : The gradient can be very small for large positive and nagitive inputs, it can lead to vanishing gradient problem.\n",
    "\n",
    "ReLU : The gradient can be either 0 or 1.\n",
    "\n",
    "3. **Zero-centered Outputs**\n",
    "\n",
    "Sigmoid : Outputs are always positive,not zero-centered, it can slow down convergence during training.\n",
    "\n",
    "ReLU : Outputs can be 0 or positive, it is also not zero-centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655c17b-486b-4f90-8fe1-35ccff933eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1abb328b-c9af-4c53-99ae-2bf80021abf1",
   "metadata": {},
   "source": [
    "## Q6. What are the benefits of using the ReLU activation function over the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f34376-4208-4546-a483-a2ff3eaa2e99",
   "metadata": {},
   "source": [
    "**1.Vanishing Gradient Problem**\n",
    "\n",
    "-> Gradient of ReLU can be zero or positive. Which solves the problem of vanishing gradient. It helps to maintain the healthy gradient flow. \n",
    "\n",
    "**2.Computation Efficiency** \n",
    "\n",
    "-> ReLU is more computationally efficient as compare to sigmoid, becasue sigmoid have a exponent in its formula.\n",
    "\n",
    "**3.Convergence Speed**\n",
    "\n",
    "-> ReLU converge faster during training due to better gradient propagation.\n",
    "\n",
    "**4.Handles Larger Inputs**\n",
    "\n",
    "-> ReLU can handle a larger range of input values without saturating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd71c5-da72-4ca1-a1f3-e54e60d84efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "593734cf-6126-41bd-8dbe-2b8b9e687471",
   "metadata": {},
   "source": [
    "## Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20ccd03-20dd-4a52-ba12-6715e4258465",
   "metadata": {},
   "source": [
    "Leaky ReLU is a varient of ReLU activation function which is build for remove some of its limitations like 'Dying ReLU', where neurons can become inactive.\n",
    "\n",
    "**Leaky ReLU Function**\n",
    "\n",
    "x if x >= 0\n",
    "\n",
    "αx if x < 0\n",
    "\n",
    "* **Addressing Vanishing Gradient Problem**\n",
    "\n",
    "-> By allowing small gradient(αx) to nagitive inputs, it prevents neurons from becoming inactive.\n",
    "\n",
    "-> It helps to maintain the healthy gradient flow throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9073483-2a20-4a80-bd73-b450f76d5134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8eebf1e0-0665-4f3b-93e8-c5ac0d64f54d",
   "metadata": {},
   "source": [
    "## Q8. What is the purpose of the softmax activation function? When is it commonly used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bebbe5-07f2-418a-abb5-8b922e9be50d",
   "metadata": {},
   "source": [
    "**-> The Softmax function helps to convert vector of values in probability distribution. It is commonly used at output layer of neural network to solve the multiclass calssification problem.**\n",
    "\n",
    "softmax(Zi) = e^Zi / sum(e^Zj)\n",
    "\n",
    "* **Common Use Cases**\n",
    "\n",
    "1. **Output Layer**\n",
    "\n",
    "-> It is commonly used at output layer to solve the problem of multi-class classification problem.\n",
    "\n",
    "2. **Loss Function**\n",
    "\n",
    "-> Softmax is also used in conjuction with cross-entropy, which measures the difference between predicted and actual distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f022f016-d988-41b3-9efe-fd3f1f2b12ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d90a93e-c59f-4134-9094-38a5de14e256",
   "metadata": {},
   "source": [
    "## Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac835ca-66a6-4b8f-bf39-0a08636bba4d",
   "metadata": {},
   "source": [
    "tanh(x) = (e^x - e^-x) / (e^x + e^-x)\n",
    "\n",
    "**Comparision**\n",
    "\n",
    "1. Range\n",
    "\n",
    "sigmoid : Ranges between 0 to 1.\n",
    "\n",
    "tanh : Ranges between -1 to 1.\n",
    "\n",
    "2. zero-centered \n",
    "\n",
    "sigmoid : It is not a zero-centered.\n",
    "\n",
    "tanh : It is a zero-centered.\n",
    "\n",
    "3. UseCase\n",
    "\n",
    "sigmoid : Mostly used at output layer for binary classification problem, where output need to be interpred as probabilities.\n",
    "\n",
    "tanh : Often used at hidden layer of the neural network, where the zero-centered outputs are benificial for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369cd162-3469-4a72-bb0f-315032a9427e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
